{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ScratchLogisticRegression():  \n",
    "    \n",
    "    def __init__(self, num_iter=100, lr=0.01, bias=False, verbose=False):\n",
    "        \n",
    "        # Record hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.lamda = 1/0.01\n",
    "        # Prepare an array to record the loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def _check_for_bias(self,x):\n",
    "        if self.bias == True:\n",
    "            x1 = np.one(x.shape[0])\n",
    "        else:\n",
    "            x1 = np.zeros(x.shape[0])\n",
    "            \n",
    "        return np.concatenate([x1.reshape(-1,1),x],axis=1)\n",
    "    \n",
    "    def _sigmoid_function(self,x):\n",
    "        linear_model = np.dot(x,self.w)\n",
    "        \n",
    "        return 1/(1+np.exp(-linear_model))\n",
    "    \n",
    "    def _gradient_descent(self, x, error):\n",
    "        self.tmp = np.append(0,np.ones(x.shape[1]-1))\n",
    "        self.w -= self.lr*(np.dot(error,x) + self.tmp*self.lamda*self.w)/len(x)\n",
    "        \n",
    "    def _loss_function(self, y, y_pred):\n",
    "        return np.mean(-y*np.log(y_pred) -(1-y)*np.log(1-y_pred))+0.5*self.lamda*np.mean(self.w[1:]**2)\n",
    "    \n",
    "    def fit(self, x, y, x_val=False, y_val=False):\n",
    "        self.ylabel = np.unique(y)\n",
    "        \n",
    "        y = np.where(y==self.ylabel[0],0,1)\n",
    "        \n",
    "        if (type(y_val) != bool):\n",
    "            y_val = np.where(y_val==self.ylabel[0],0,1)\n",
    "            \n",
    "        x = self._check_for_bias(x)\n",
    "        \n",
    "        self.w = np.random.rand(x.shape[1])\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            y_pred = self._sigmoid_function(x)\n",
    "            error = y_pred - y\n",
    "            self.loss[i] = self._loss_function(y,y_pred)\n",
    "            \n",
    "            if (type(x_val) != bool):\n",
    "                val_x = self._check_for_bias(x_val)\n",
    "                val_ypred = self._sigmoid_function(val_x)\n",
    "                \n",
    "                self.val_loss[i] = self._loss_function(y_val,val_ypred)\n",
    "                \n",
    "            self._gradient_descent(x, error)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('n_iter:', i,\n",
    "                     'loss:',self.loss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = load_iris().data\n",
    "target = load_iris().target.reshape(-1,1)\n",
    "iris = np.concatenate([data,target],axis=1)\n",
    "iris = pd.DataFrame(iris)\n",
    "x = iris.loc[iris[4]!=0,2:3].values\n",
    "y = iris.loc[iris[4]!=0,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter: 0 loss: 13.99330738781123\n",
      "n_iter: 1 loss: 13.754090185939113\n",
      "n_iter: 2 loss: 13.520635416089654\n",
      "n_iter: 3 loss: 13.292797255830415\n",
      "n_iter: 4 loss: 13.070433668306705\n",
      "n_iter: 5 loss: 12.853406304331926\n",
      "n_iter: 6 loss: 12.641580406912507\n",
      "n_iter: 7 loss: 12.434824718151807\n",
      "n_iter: 8 loss: 12.233011388478264\n",
      "n_iter: 9 loss: 12.036015888144357\n",
      "n_iter: 10 loss: 11.843716920943987\n",
      "n_iter: 11 loss: 11.655996340096971\n",
      "n_iter: 12 loss: 11.472739066250302\n",
      "n_iter: 13 loss: 11.29383300754704\n",
      "n_iter: 14 loss: 11.119168981714491\n",
      "n_iter: 15 loss: 10.948640640124516\n",
      "n_iter: 16 loss: 10.78214439377959\n",
      "n_iter: 17 loss: 10.619579341179387\n",
      "n_iter: 18 loss: 10.460847198023393\n",
      "n_iter: 19 loss: 10.305852228706138\n",
      "n_iter: 20 loss: 10.154501179562457\n",
      "n_iter: 21 loss: 10.006703213821012\n",
      "n_iter: 22 loss: 9.862369848225352\n",
      "n_iter: 23 loss: 9.721414891282462\n",
      "n_iter: 24 loss: 9.583754383099674\n",
      "n_iter: 25 loss: 9.449306536771665\n",
      "n_iter: 26 loss: 9.31799168127999\n",
      "n_iter: 27 loss: 9.189732205868456\n",
      "n_iter: 28 loss: 9.064452505858451\n",
      "n_iter: 29 loss: 8.942078929869012\n",
      "n_iter: 30 loss: 8.82253972840728\n",
      "n_iter: 31 loss: 8.705765003795662\n",
      "n_iter: 32 loss: 8.591686661402791\n",
      "n_iter: 33 loss: 8.480238362146046\n",
      "n_iter: 34 loss: 8.371355476234195\n",
      "n_iter: 35 loss: 8.264975038119264\n",
      "n_iter: 36 loss: 8.161035702627593\n",
      "n_iter: 37 loss: 8.059477702240587\n",
      "n_iter: 38 loss: 7.960242805496329\n",
      "n_iter: 39 loss: 7.863274276483993\n",
      "n_iter: 40 loss: 7.768516835403437\n",
      "n_iter: 41 loss: 7.675916620163193\n",
      "n_iter: 42 loss: 7.58542114899047\n",
      "n_iter: 43 loss: 7.496979284027584\n",
      "n_iter: 44 loss: 7.410541195889663\n",
      "n_iter: 45 loss: 7.326058329159158\n",
      "n_iter: 46 loss: 7.24348336879321\n",
      "n_iter: 47 loss: 7.1627702074204915\n",
      "n_iter: 48 loss: 7.083873913504697\n",
      "n_iter: 49 loss: 7.006750700352382\n",
      "n_iter: 50 loss: 6.93135789594338\n",
      "n_iter: 51 loss: 6.857653913562578\n",
      "n_iter: 52 loss: 6.785598223212241\n",
      "n_iter: 53 loss: 6.715151323784715\n",
      "n_iter: 54 loss: 6.646274715975697\n",
      "n_iter: 55 loss: 6.578930875918796\n",
      "n_iter: 56 loss: 6.513083229522589\n",
      "n_iter: 57 loss: 6.448696127491796\n",
      "n_iter: 58 loss: 6.385734821014664\n",
      "n_iter: 59 loss: 6.324165438099101\n",
      "n_iter: 60 loss: 6.26395496054051\n",
      "n_iter: 61 loss: 6.2050712015047065\n",
      "n_iter: 62 loss: 6.147482783709704\n",
      "n_iter: 63 loss: 6.09115911819058\n",
      "n_iter: 64 loss: 6.036070383631997\n",
      "n_iter: 65 loss: 5.982187506253353\n",
      "n_iter: 66 loss: 5.929482140231934\n",
      "n_iter: 67 loss: 5.877926648649761\n",
      "n_iter: 68 loss: 5.827494084950249\n",
      "n_iter: 69 loss: 5.77815817489108\n",
      "n_iter: 70 loss: 5.729893298980096\n",
      "n_iter: 71 loss: 5.682674475381329\n",
      "n_iter: 72 loss: 5.636477343278591\n",
      "n_iter: 73 loss: 5.591278146684425\n",
      "n_iter: 74 loss: 5.5470537186824895\n",
      "n_iter: 75 loss: 5.503781466091763\n",
      "n_iter: 76 loss: 5.461439354541268\n",
      "n_iter: 77 loss: 5.420005893944285\n",
      "n_iter: 78 loss: 5.379460124361359\n",
      "n_iter: 79 loss: 5.339781602241616\n",
      "n_iter: 80 loss: 5.300950387032225\n",
      "n_iter: 81 loss: 5.262947028146104\n",
      "n_iter: 82 loss: 5.225752552278204\n",
      "n_iter: 83 loss: 5.189348451060983\n",
      "n_iter: 84 loss: 5.1537166690499285\n",
      "n_iter: 85 loss: 5.118839592030193\n",
      "n_iter: 86 loss: 5.084700035635696\n",
      "n_iter: 87 loss: 5.051281234272242\n",
      "n_iter: 88 loss: 5.018566830336434\n",
      "n_iter: 89 loss: 4.986540863722366\n",
      "n_iter: 90 loss: 4.955187761608342\n",
      "n_iter: 91 loss: 4.924492328516002\n",
      "n_iter: 92 loss: 4.8944397366345065\n",
      "n_iter: 93 loss: 4.8650155164025835\n",
      "n_iter: 94 loss: 4.8362055473414465\n",
      "n_iter: 95 loss: 4.807996049131805\n",
      "n_iter: 96 loss: 4.780373572928314\n",
      "n_iter: 97 loss: 4.753324992905061\n",
      "n_iter: 98 loss: 4.7268374980257875\n",
      "n_iter: 99 loss: 4.700898584032775\n",
      "n_iter: 100 loss: 4.675496045648441\n",
      "n_iter: 101 loss: 4.650617968983901\n",
      "n_iter: 102 loss: 4.626252724148826\n",
      "n_iter: 103 loss: 4.60238895805721\n",
      "n_iter: 104 loss: 4.579015587423639\n",
      "n_iter: 105 loss: 4.55612179194498\n",
      "n_iter: 106 loss: 4.53369700766238\n",
      "n_iter: 107 loss: 4.511730920498738\n",
      "n_iter: 108 loss: 4.490213459966853\n",
      "n_iter: 109 loss: 4.469134793043626\n",
      "n_iter: 110 loss: 4.448485318205821\n",
      "n_iter: 111 loss: 4.428255659622985\n",
      "n_iter: 112 loss: 4.408436661503275\n",
      "n_iter: 113 loss: 4.389019382588039\n",
      "n_iter: 114 loss: 4.369995090791117\n",
      "n_iter: 115 loss: 4.351355257978952\n",
      "n_iter: 116 loss: 4.33309155488767\n",
      "n_iter: 117 loss: 4.315195846173439\n",
      "n_iter: 118 loss: 4.297660185592489\n",
      "n_iter: 119 loss: 4.280476811307289\n",
      "n_iter: 120 loss: 4.2636381413154405\n",
      "n_iter: 121 loss: 4.247136768998006\n",
      "n_iter: 122 loss: 4.230965458784016\n",
      "n_iter: 123 loss: 4.215117141928016\n",
      "n_iter: 124 loss: 4.1995849123975955\n",
      "n_iter: 125 loss: 4.184362022867954\n",
      "n_iter: 126 loss: 4.169441880820574\n",
      "n_iter: 127 loss: 4.154818044743218\n",
      "n_iter: 128 loss: 4.140484220428514\n",
      "n_iter: 129 loss: 4.126434257368456\n",
      "n_iter: 130 loss: 4.112662145242256\n",
      "n_iter: 131 loss: 4.099162010495017\n",
      "n_iter: 132 loss: 4.085928113004784\n",
      "n_iter: 133 loss: 4.072954842835615\n",
      "n_iter: 134 loss: 4.060236717074324\n",
      "n_iter: 135 loss: 4.047768376748684\n",
      "n_iter: 136 loss: 4.035544583824874\n",
      "n_iter: 137 loss: 4.023560218282065\n",
      "n_iter: 138 loss: 4.01181027526207\n",
      "n_iter: 139 loss: 4.0002898622920275\n",
      "n_iter: 140 loss: 3.9889941965782025\n",
      "n_iter: 141 loss: 3.977918602368962\n",
      "n_iter: 142 loss: 3.967058508385097\n",
      "n_iter: 143 loss: 3.9564094453156926\n",
      "n_iter: 144 loss: 3.9459670433777805\n",
      "n_iter: 145 loss: 3.935727029938092\n",
      "n_iter: 146 loss: 3.925685227195242\n",
      "n_iter: 147 loss: 3.9158375499207354\n",
      "n_iter: 148 loss: 3.9061800032572442\n",
      "n_iter: 149 loss: 3.896708680572611\n",
      "n_iter: 150 loss: 3.8874197613681165\n",
      "n_iter: 151 loss: 3.878309509239566\n",
      "n_iter: 152 loss: 3.869374269889782\n",
      "n_iter: 153 loss: 3.8606104691911627\n",
      "n_iter: 154 loss: 3.852014611296959\n",
      "n_iter: 155 loss: 3.843583276799999\n",
      "n_iter: 156 loss: 3.835313120937595\n",
      "n_iter: 157 loss: 3.8272008718414146\n",
      "n_iter: 158 loss: 3.8192433288311447\n",
      "n_iter: 159 loss: 3.811437360750765\n",
      "n_iter: 160 loss: 3.803779904346344\n",
      "n_iter: 161 loss: 3.7962679626842393\n",
      "n_iter: 162 loss: 3.7888986036086516\n",
      "n_iter: 163 loss: 3.7816689582375025\n",
      "n_iter: 164 loss: 3.774576219495625\n",
      "n_iter: 165 loss: 3.767617640684292\n",
      "n_iter: 166 loss: 3.7607905340861274\n",
      "n_iter: 167 loss: 3.754092269604492\n",
      "n_iter: 168 loss: 3.747520273436418\n",
      "n_iter: 169 loss: 3.7410720267782454\n",
      "n_iter: 170 loss: 3.734745064563083\n",
      "n_iter: 171 loss: 3.728536974229282\n",
      "n_iter: 172 loss: 3.7224453945191156\n",
      "n_iter: 173 loss: 3.7164680143068707\n",
      "n_iter: 174 loss: 3.710602571455601\n",
      "n_iter: 175 loss: 3.704846851701785\n",
      "n_iter: 176 loss: 3.699198687567188\n",
      "n_iter: 177 loss: 3.6936559572971985\n",
      "n_iter: 178 loss: 3.688216583824981\n",
      "n_iter: 179 loss: 3.6828785337607615\n",
      "n_iter: 180 loss: 3.6776398164056068\n",
      "n_iter: 181 loss: 3.672498482789069\n",
      "n_iter: 182 loss: 3.6674526247300854\n",
      "n_iter: 183 loss: 3.662500373920511\n",
      "n_iter: 184 loss: 3.657639901030751\n",
      "n_iter: 185 loss: 3.6528694148368794\n",
      "n_iter: 186 loss: 3.6481871613687327\n",
      "n_iter: 187 loss: 3.6435914230784157\n",
      "n_iter: 188 loss: 3.639080518028714\n",
      "n_iter: 189 loss: 3.634652799100907\n",
      "n_iter: 190 loss: 3.6303066532214783\n",
      "n_iter: 191 loss: 3.6260405006072496\n",
      "n_iter: 192 loss: 3.621852794028477\n",
      "n_iter: 193 loss: 3.6177420180894413\n",
      "n_iter: 194 loss: 3.613706688526099\n",
      "n_iter: 195 loss: 3.6097453515203632\n",
      "n_iter: 196 loss: 3.605856583030593\n",
      "n_iter: 197 loss: 3.6020389881378794\n",
      "n_iter: 198 loss: 3.5982912004077345\n",
      "n_iter: 199 loss: 3.5946118812668004\n",
      "n_iter: 200 loss: 3.590999719394196\n",
      "n_iter: 201 loss: 3.5874534301271286\n",
      "n_iter: 202 loss: 3.583971754880434\n",
      "n_iter: 203 loss: 3.5805534605796687\n",
      "n_iter: 204 loss: 3.577197339107448\n",
      "n_iter: 205 loss: 3.573902206762668\n",
      "n_iter: 206 loss: 3.570666903732318\n",
      "n_iter: 207 loss: 3.5674902935755513\n",
      "n_iter: 208 loss: 3.564371262719721\n",
      "n_iter: 209 loss: 3.5613087199680824\n",
      "n_iter: 210 loss: 3.5583015960188717\n",
      "n_iter: 211 loss: 3.555348842995479\n",
      "n_iter: 212 loss: 3.5524494339874453\n",
      "n_iter: 213 loss: 3.5496023626020166\n",
      "n_iter: 214 loss: 3.546806642525989\n",
      "n_iter: 215 loss: 3.5440613070976053\n",
      "n_iter: 216 loss: 3.5413654088882405\n",
      "n_iter: 217 loss: 3.538718019293655\n",
      "n_iter: 218 loss: 3.536118228134557\n",
      "n_iter: 219 loss: 3.5335651432662747\n",
      "n_iter: 220 loss: 3.531057890197295\n",
      "n_iter: 221 loss: 3.5285956117164643\n",
      "n_iter: 222 loss: 3.526177467528636\n",
      "n_iter: 223 loss: 3.52380263389856\n",
      "n_iter: 224 loss: 3.521470303302819\n",
      "n_iter: 225 loss: 3.5191796840896092\n",
      "n_iter: 226 loss: 3.516930000146184\n",
      "n_iter: 227 loss: 3.5147204905737697\n",
      "n_iter: 228 loss: 3.512550409369778\n",
      "n_iter: 229 loss: 3.510419025117135\n",
      "n_iter: 230 loss: 3.508325620680563\n",
      "n_iter: 231 loss: 3.506269492909637\n",
      "n_iter: 232 loss: 3.504249952348469\n",
      "n_iter: 233 loss: 3.5022663229518485\n",
      "n_iter: 234 loss: 3.5003179418076864\n",
      "n_iter: 235 loss: 3.4984041588656307\n",
      "n_iter: 236 loss: 3.4965243366716754\n",
      "n_iter: 237 loss: 3.4946778501086433\n",
      "n_iter: 238 loss: 3.4928640861424074\n",
      "n_iter: 239 loss: 3.4910824435736894\n",
      "n_iter: 240 loss: 3.48933233279533\n",
      "n_iter: 241 loss: 3.48761317555489\n",
      "n_iter: 242 loss: 3.4859244047224505\n",
      "n_iter: 243 loss: 3.484265464063509\n",
      "n_iter: 244 loss: 3.482635808016833\n",
      "n_iter: 245 loss: 3.4810349014771598\n",
      "n_iter: 246 loss: 3.4794622195826346\n",
      "n_iter: 247 loss: 3.4779172475068796\n",
      "n_iter: 248 loss: 3.476399480255563\n",
      "n_iter: 249 loss: 3.4749084224673954\n",
      "n_iter: 250 loss: 3.473443588219422\n",
      "n_iter: 251 loss: 3.472004500836533\n",
      "n_iter: 252 loss: 3.4705906927050814\n",
      "n_iter: 253 loss: 3.469201705090513\n",
      "n_iter: 254 loss: 3.4678370879589258\n",
      "n_iter: 255 loss: 3.4664963998024505\n",
      "n_iter: 256 loss: 3.4651792074683954\n",
      "n_iter: 257 loss: 3.4638850859920267\n",
      "n_iter: 258 loss: 3.4626136184329375\n",
      "n_iter: 259 loss: 3.461364395714904\n",
      "n_iter: 260 loss: 3.4601370164691545\n",
      "n_iter: 261 loss: 3.4589310868809715\n",
      "n_iter: 262 loss: 3.4577462205395535\n",
      "n_iter: 263 loss: 3.456582038291062\n",
      "n_iter: 264 loss: 3.4554381680947754\n",
      "n_iter: 265 loss: 3.4543142448822897\n",
      "n_iter: 266 loss: 3.4532099104196874\n",
      "n_iter: 267 loss: 3.4521248131726114\n",
      "n_iter: 268 loss: 3.4510586081741788\n",
      "n_iter: 269 loss: 3.4500109568956723\n",
      "n_iter: 270 loss: 3.448981527119937\n",
      "n_iter: 271 loss: 3.4479699928174403\n",
      "n_iter: 272 loss: 3.446976034024908\n",
      "n_iter: 273 loss: 3.4459993367265143\n",
      "n_iter: 274 loss: 3.445039592737529\n",
      "n_iter: 275 loss: 3.4440964995904024\n",
      "n_iter: 276 loss: 3.443169760423209\n",
      "n_iter: 277 loss: 3.44225908387041\n",
      "n_iter: 278 loss: 3.44136418395588\n",
      "n_iter: 279 loss: 3.440484779988145\n",
      "n_iter: 280 loss: 3.4396205964577846\n",
      "n_iter: 281 loss: 3.438771362936958\n",
      "n_iter: 282 loss: 3.437936813980988\n",
      "n_iter: 283 loss: 3.4371166890319804\n",
      "n_iter: 284 loss: 3.4363107323244155\n",
      "n_iter: 285 loss: 3.435518692792677\n",
      "n_iter: 286 loss: 3.434740323980482\n",
      "n_iter: 287 loss: 3.433975383952151\n",
      "n_iter: 288 loss: 3.4332236352056973\n",
      "n_iter: 289 loss: 3.432484844587691\n",
      "n_iter: 290 loss: 3.431758783209848\n",
      "n_iter: 291 loss: 3.4310452263673215\n",
      "n_iter: 292 loss: 3.4303439534586477\n",
      "n_iter: 293 loss: 3.4296547479073203\n",
      "n_iter: 294 loss: 3.4289773970849478\n",
      "n_iter: 295 loss: 3.4283116922359644\n",
      "n_iter: 296 loss: 3.4276574284038617\n",
      "n_iter: 297 loss: 3.4270144043589097\n",
      "n_iter: 298 loss: 3.4263824225273316\n",
      "n_iter: 299 loss: 3.425761288921897\n",
      "n_iter: 300 loss: 3.425150813073916\n",
      "n_iter: 301 loss: 3.424550807966593\n",
      "n_iter: 302 loss: 3.4239610899697164\n",
      "n_iter: 303 loss: 3.4233814787756485\n",
      "n_iter: 304 loss: 3.4228117973366037\n",
      "n_iter: 305 loss: 3.422251871803166\n",
      "n_iter: 306 loss: 3.4217015314640364\n",
      "n_iter: 307 loss: 3.4211606086869804\n",
      "n_iter: 308 loss: 3.420628938860941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter: 309 loss: 3.4201063603393074\n",
      "n_iter: 310 loss: 3.4195927143843017\n",
      "n_iter: 311 loss: 3.4190878451124673\n",
      "n_iter: 312 loss: 3.4185915994412355\n",
      "n_iter: 313 loss: 3.418103827036543\n",
      "n_iter: 314 loss: 3.4176243802614854\n",
      "n_iter: 315 loss: 3.4171531141259743\n",
      "n_iter: 316 loss: 3.416689886237394\n",
      "n_iter: 317 loss: 3.4162345567522143\n",
      "n_iter: 318 loss: 3.4157869883285628\n",
      "n_iter: 319 loss: 3.4153470460797157\n",
      "n_iter: 320 loss: 3.4149145975285067\n",
      "n_iter: 321 loss: 3.4144895125626196\n",
      "n_iter: 322 loss: 3.414071663390755\n",
      "n_iter: 323 loss: 3.4136609244996534\n",
      "n_iter: 324 loss: 3.4132571726119476\n",
      "n_iter: 325 loss: 3.41286028664484\n",
      "n_iter: 326 loss: 3.4124701476695805\n",
      "n_iter: 327 loss: 3.4120866388717266\n",
      "n_iter: 328 loss: 3.411709645512179\n",
      "n_iter: 329 loss: 3.41133905488896\n",
      "n_iter: 330 loss: 3.410974756299743\n",
      "n_iter: 331 loss: 3.4106166410050953\n",
      "n_iter: 332 loss: 3.4102646021924317\n",
      "n_iter: 333 loss: 3.4099185349406618\n",
      "n_iter: 334 loss: 3.409578336185519\n",
      "n_iter: 335 loss: 3.4092439046855505\n",
      "n_iter: 336 loss: 3.4089151409887624\n",
      "n_iter: 337 loss: 3.4085919473999042\n",
      "n_iter: 338 loss: 3.4082742279483753\n",
      "n_iter: 339 loss: 3.407961888356752\n",
      "n_iter: 340 loss: 3.407654836009905\n",
      "n_iter: 341 loss: 3.4073529799247155\n",
      "n_iter: 342 loss: 3.4070562307203645\n",
      "n_iter: 343 loss: 3.406764500589182\n",
      "n_iter: 344 loss: 3.4064777032680587\n",
      "n_iter: 345 loss: 3.406195754010396\n",
      "n_iter: 346 loss: 3.4059185695585867\n",
      "n_iter: 347 loss: 3.4056460681170186\n",
      "n_iter: 348 loss: 3.4053781693255902\n",
      "n_iter: 349 loss: 3.4051147942337248\n",
      "n_iter: 350 loss: 3.4048558652748726\n",
      "n_iter: 351 loss: 3.4046013062415037\n",
      "n_iter: 352 loss: 3.404351042260556\n",
      "n_iter: 353 loss: 3.404104999769365\n",
      "n_iter: 354 loss: 3.403863106492025\n",
      "n_iter: 355 loss: 3.403625291416212\n",
      "n_iter: 356 loss: 3.403391484770431\n",
      "n_iter: 357 loss: 3.403161618001694\n",
      "n_iter: 358 loss: 3.4029356237536144\n",
      "n_iter: 359 loss: 3.40271343584491\n",
      "n_iter: 360 loss: 3.402494989248311\n",
      "n_iter: 361 loss: 3.40228022006986\n",
      "n_iter: 362 loss: 3.4020690655285963\n",
      "n_iter: 363 loss: 3.401861463936621\n",
      "n_iter: 364 loss: 3.4016573546795303\n",
      "n_iter: 365 loss: 3.4014566781972198\n",
      "n_iter: 366 loss: 3.401259375965032\n",
      "n_iter: 367 loss: 3.401065390475271\n",
      "n_iter: 368 loss: 3.400874665219042\n",
      "n_iter: 369 loss: 3.400687144668442\n",
      "n_iter: 370 loss: 3.4005027742590714\n",
      "n_iter: 371 loss: 3.4003215003728693\n",
      "n_iter: 372 loss: 3.4001432703212724\n",
      "n_iter: 373 loss: 3.3999680323286774\n",
      "n_iter: 374 loss: 3.39979573551621\n",
      "n_iter: 375 loss: 3.3996263298857974\n",
      "n_iter: 376 loss: 3.399459766304527\n",
      "n_iter: 377 loss: 3.3992959964892977\n",
      "n_iter: 378 loss: 3.3991349729917504\n",
      "n_iter: 379 loss: 3.3989766491834796\n",
      "n_iter: 380 loss: 3.398820979241508\n",
      "n_iter: 381 loss: 3.3986679181340333\n",
      "n_iter: 382 loss: 3.3985174216064395\n",
      "n_iter: 383 loss: 3.3983694461675515\n",
      "n_iter: 384 loss: 3.3982239490761557\n",
      "n_iter: 385 loss: 3.3980808883277533\n",
      "n_iter: 386 loss: 3.3979402226415663\n",
      "n_iter: 387 loss: 3.3978019114477735\n",
      "n_iter: 388 loss: 3.397665914874982\n",
      "n_iter: 389 loss: 3.3975321937379253\n",
      "n_iter: 390 loss: 3.397400709525384\n",
      "n_iter: 391 loss: 3.397271424388331\n",
      "n_iter: 392 loss: 3.3971443011282862\n",
      "n_iter: 393 loss: 3.3970193031858864\n",
      "n_iter: 394 loss: 3.3968963946296555\n",
      "n_iter: 395 loss: 3.396775540144991\n",
      "n_iter: 396 loss: 3.396656705023336\n",
      "n_iter: 397 loss: 3.396539855151557\n",
      "n_iter: 398 loss: 3.396424957001506\n",
      "n_iter: 399 loss: 3.3963119776197788\n",
      "n_iter: 400 loss: 3.39620088461765\n",
      "n_iter: 401 loss: 3.3960916461611967\n",
      "n_iter: 402 loss: 3.395984230961594\n",
      "n_iter: 403 loss: 3.3958786082655923\n",
      "n_iter: 404 loss: 3.395774747846155\n",
      "n_iter: 405 loss: 3.3956726199932783\n",
      "n_iter: 406 loss: 3.3955721955049643\n",
      "n_iter: 407 loss: 3.395473445678361\n",
      "n_iter: 408 loss: 3.395376342301062\n",
      "n_iter: 409 loss: 3.39528085764256\n",
      "n_iter: 410 loss: 3.3951869644458554\n",
      "n_iter: 411 loss: 3.395094635919211\n",
      "n_iter: 412 loss: 3.39500384572806\n",
      "n_iter: 413 loss: 3.3949145679870547\n",
      "n_iter: 414 loss: 3.394826777252259\n",
      "n_iter: 415 loss: 3.394740448513477\n",
      "n_iter: 416 loss: 3.3946555571867236\n",
      "n_iter: 417 loss: 3.3945720791068252\n",
      "n_iter: 418 loss: 3.394489990520154\n",
      "n_iter: 419 loss: 3.3944092680774913\n",
      "n_iter: 420 loss: 3.3943298888270172\n",
      "n_iter: 421 loss: 3.394251830207426\n",
      "n_iter: 422 loss: 3.3941750700411624\n",
      "n_iter: 423 loss: 3.39409958652778\n",
      "n_iter: 424 loss: 3.394025358237416\n",
      "n_iter: 425 loss: 3.3939523641043796\n",
      "n_iter: 426 loss: 3.393880583420861\n",
      "n_iter: 427 loss: 3.393809995830744\n",
      "n_iter: 428 loss: 3.3937405813235304\n",
      "n_iter: 429 loss: 3.3936723202283754\n",
      "n_iter: 430 loss: 3.3936051932082245\n",
      "n_iter: 431 loss: 3.393539181254057\n",
      "n_iter: 432 loss: 3.3934742656792247\n",
      "n_iter: 433 loss: 3.393410428113901\n",
      "n_iter: 434 loss: 3.393347650499619\n",
      "n_iter: 435 loss: 3.3932859150839096\n",
      "n_iter: 436 loss: 3.3932252044150326\n",
      "n_iter: 437 loss: 3.3931655013368047\n",
      "n_iter: 438 loss: 3.3931067889835105\n",
      "n_iter: 439 loss: 3.3930490507749105\n",
      "n_iter: 440 loss: 3.3929922704113347\n",
      "n_iter: 441 loss: 3.39293643186886\n",
      "n_iter: 442 loss: 3.3928815193945745\n",
      "n_iter: 443 loss: 3.392827517501925\n",
      "n_iter: 444 loss: 3.392774410966144\n",
      "n_iter: 445 loss: 3.3927221848197613\n",
      "n_iter: 446 loss: 3.392670824348188\n",
      "n_iter: 447 loss: 3.3926203150853826\n",
      "n_iter: 448 loss: 3.3925706428095923\n",
      "n_iter: 449 loss: 3.3925217935391667\n",
      "n_iter: 450 loss: 3.3924737535284457\n",
      "n_iter: 451 loss: 3.3924265092637196\n",
      "n_iter: 452 loss: 3.3923800474592625\n",
      "n_iter: 453 loss: 3.3923343550534266\n",
      "n_iter: 454 loss: 3.3922894192048147\n",
      "n_iter: 455 loss: 3.3922452272885124\n",
      "n_iter: 456 loss: 3.3922017668923914\n",
      "n_iter: 457 loss: 3.392159025813469\n",
      "n_iter: 458 loss: 3.392116992054342\n",
      "n_iter: 459 loss: 3.3920756538196715\n",
      "n_iter: 460 loss: 3.3920349995127395\n",
      "n_iter: 461 loss: 3.3919950177320555\n",
      "n_iter: 462 loss: 3.3919556972680307\n",
      "n_iter: 463 loss: 3.3919170270997023\n",
      "n_iter: 464 loss: 3.3918789963915197\n",
      "n_iter: 465 loss: 3.3918415944901867\n",
      "n_iter: 466 loss: 3.391804810921554\n",
      "n_iter: 467 loss: 3.3917686353875713\n",
      "n_iter: 468 loss: 3.391733057763288\n",
      "n_iter: 469 loss: 3.3916980680939046\n",
      "n_iter: 470 loss: 3.3916636565918843\n",
      "n_iter: 471 loss: 3.3916298136341023\n",
      "n_iter: 472 loss: 3.391596529759052\n",
      "n_iter: 473 loss: 3.391563795664098\n",
      "n_iter: 474 loss: 3.3915316022027775\n",
      "n_iter: 475 loss: 3.391499940382144\n",
      "n_iter: 476 loss: 3.391468801360164\n",
      "n_iter: 477 loss: 3.3914381764431507\n",
      "n_iter: 478 loss: 3.3914080570832486\n",
      "n_iter: 479 loss: 3.391378434875959\n",
      "n_iter: 480 loss: 3.391349301557706\n",
      "n_iter: 481 loss: 3.3913206490034478\n",
      "n_iter: 482 loss: 3.391292469224328\n",
      "n_iter: 483 loss: 3.391264754365368\n",
      "n_iter: 484 loss: 3.3912374967031953\n",
      "n_iter: 485 loss: 3.3912106886438167\n",
      "n_iter: 486 loss: 3.3911843227204272\n",
      "n_iter: 487 loss: 3.3911583915912535\n",
      "n_iter: 488 loss: 3.3911328880374407\n",
      "n_iter: 489 loss: 3.3911078049609698\n",
      "n_iter: 490 loss: 3.3910831353826154\n",
      "n_iter: 491 loss: 3.3910588724399355\n",
      "n_iter: 492 loss: 3.3910350093852935\n",
      "n_iter: 493 loss: 3.3910115395839235\n",
      "n_iter: 494 loss: 3.3909884565120176\n",
      "n_iter: 495 loss: 3.390965753754852\n",
      "n_iter: 496 loss: 3.390943425004946\n",
      "n_iter: 497 loss: 3.3909214640602476\n",
      "n_iter: 498 loss: 3.390899864822357\n",
      "n_iter: 499 loss: 3.3908786212947746\n"
     ]
    }
   ],
   "source": [
    "model = ScratchLogisticRegression(num_iter=500, lr=0.01, verbose=True)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "plt.scatter(range(len(model.loss)),model.loss,label='Train loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLogisticRegression():  \n",
    "    \n",
    "    def __init__(self, num_iter=100, lr=0.01, bias=False, verbose=False):\n",
    "        \n",
    "        # Record hyperparameters as attributes\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        self.lamda = 1/0.01\n",
    "        # Prepare an array to record the loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def _check_for_bias(self,x):\n",
    "        if self.bias == True:\n",
    "            x1 = np.one(x.shape[0])\n",
    "        else:\n",
    "            x1 = np.zeros(x.shape[0])\n",
    "            \n",
    "        return np.concatenate([x1.reshape(-1,1),x],axis=1)\n",
    "    \n",
    "    def _sigmoid_function(self,x):\n",
    "        linear_model = np.dot(x,self.w)\n",
    "        \n",
    "        return 1/(1+np.exp(-linear_model))\n",
    "    \n",
    "    def _gradient_descent(self, x, error):\n",
    "        self.tmp = np.append(0,np.ones(x.shape[1]-1))\n",
    "        self.w -= self.lr*(np.dot(error,x) + self.tmp*self.lamda*self.w)/len(x)\n",
    "        \n",
    "    def _loss_function(self, y, y_pred):\n",
    "        return np.mean(-y*np.log(y_pred) -(1-y)*np.log(1-y_pred))+0.5*self.lamda*np.mean(self.w[1:]**2)\n",
    "    \n",
    "    def fit(self, x, y, x_val=False, y_val=False):\n",
    "        self.ylabel = np.unique(y)\n",
    "        \n",
    "        y = np.where(y==self.ylabel[0],0,1)\n",
    "        \n",
    "        if (type(y_val) != bool):\n",
    "            y_val = np.where(y_val==self.ylabel[0],0,1)\n",
    "            \n",
    "        x = self._check_for_bias(x)\n",
    "        \n",
    "        self.w = np.random.rand(x.shape[1])\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            y_pred = self._sigmoid_function(x)\n",
    "            error = y_pred - y\n",
    "            self.loss[i] = self._loss_function(y,y_pred)\n",
    "            \n",
    "            if (type(x_val) != bool):\n",
    "                val_x = self._check_for_bias(x_val)\n",
    "                val_ypred = self._sigmoid_function(val_x)\n",
    "                \n",
    "                self.val_loss[i] = self._loss_function(y_val,val_ypred)\n",
    "                \n",
    "            self._gradient_descent(x, error)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print('n_iter:', i,\n",
    "                     'loss:',self.loss[i])\n",
    "                \n",
    "    def predict(self, x):\n",
    "            x = self._check_for_bias(x)\n",
    "            y_pred = self._sigmoid_function(x)\n",
    "            return np.where(y_pred<0.5,self.ylabel[0],self.ylabel[1])\n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "            x = self._check_for_bias(x)\n",
    "            self._sigmoid_function(x)\n",
    "            return self._sigmoid_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = ScratchLogisticRegression(num_iter=1000, lr=0.001,  verbose=False)\n",
    "model.fit(x_train,y_train)\n",
    "label_pred = model.predict(x_test)\n",
    "proba_pred = model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "accuracy 0.5\n",
      "precision 0.25\n",
      "recall 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SULCOTT\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, label_pred)\n",
    "precision = precision_score(y_test, label_pred, average='weighted')\n",
    "recall = recall_score(y_test, label_pred, average='weighted')\n",
    "\n",
    "print(\"-----------\")\n",
    "print(\"accuracy\", accuracy)\n",
    "print(\"precision\", precision)\n",
    "print(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_region(x, y, model, step=0.01, title='decision region', xlabel='xlabel', ylabel='ylabel', target_names=['versicolor','virginica']):\n",
    "    \n",
    "    #setting\n",
    "    scatter_color = ['red', 'blue']\n",
    "    contourf_color = ['pink', 'skyblue']\n",
    "    n_class = 2\n",
    "    \n",
    "    # pred\n",
    "    mesh_f0, mesh_f1 = np.meshgrid(np.min(x[:,0])-0.5, np.max(x[:,0])+0.5, step), np.arange(np.min(x[:,1])-0.5, np.max(x[:,1])+0.5, step)\n",
    "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
    "    y_pred = model.predict(mesh).reshape(mesh_f0.shape)\n",
    "    \n",
    "    #plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.contourf(mesh_f0, mesh_f1, y_pred, n_class-1, cmap=ListedColormap(contourf_color))\n",
    "    plt.contour(mesh_f0, mesh_f1, y_pred, n_class-1, colors='y', linewidths=3, alpha=0.5)\n",
    "    for i, target in enumerate(set(y)):\n",
    "        plt.scatter(x[y==target][:, 0], x[y==target][:, 1], s=80, color=scatter_color[i], label=target_names[i], marker='o')\n",
    "    patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "    plt.legend(handles=patches)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit,ax = plt.subplots(figsize=(16,9))\n",
    "decision_region(x,y,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
